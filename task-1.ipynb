{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries\n",
        "It is important to note that the code needs 3.4.1 version of keras to run.\n",
        "\n",
        "To quickly run the notepbook, just run all the cells in order. A detailed explanation of the code and approach is provided on the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UFvIilnNmKV",
        "outputId": "4ca96e68-c228-4cbd-edaf-1d0e7da27d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==3.4.1 in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras==3.4.1) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.4.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.4.1) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.4.1) (0.1.2)\n",
            "Collecting cairocffi\n",
            "  Downloading cairocffi-1.7.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from cairocffi) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.1.0->cairocffi) (2.22)\n",
            "Downloading cairocffi-1.7.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cairocffi\n",
            "Successfully installed cairocffi-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install keras==3.4.1\n",
        "!pip install cairocffi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "doGSb-ynNmKX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "import tensorflow as tf\n",
        "import cairocffi as cairo\n",
        "import json\n",
        "import cv2\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "import pandas as pd\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.interpolate as si\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re4zHqg_R1fD",
        "outputId": "6645498a-baa3-40b6-fe5b-67323901ff6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.4.1\n"
          ]
        }
      ],
      "source": [
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1zTaF66rNmKY"
      },
      "outputs": [],
      "source": [
        "#  Shape dictionary\n",
        "shape_dict = {\n",
        "    0: 'line',\n",
        "    1: 'circle',\n",
        "    2: 'ellipse',\n",
        "    3: 'triangle',\n",
        "    4: 'square',\n",
        "    5: 'rounded_rectangle',\n",
        "    6: 'pentagon',\n",
        "    7: 'hexagon',\n",
        "    8: 'octagon',\n",
        "    9: 'cloud',\n",
        "    10: 'star',\n",
        "    11: 'curve'\n",
        "}\n",
        "\n",
        "side = 256\n",
        "small_side = 28\n",
        "\n",
        "nb_classes = len(shape_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task- I: Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class to represent a polyline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TGlH_Mb0NmKY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@original_polylines: the original polylines\n",
        "@polylines: polylines that will be manipulated through the prediction process\n",
        "@lines_used: the number of polylines in the subset\n",
        "@probability: the probability of the predicted class\n",
        "@predicted_class: the predicted class\n",
        "@predicted_class_label: the predicted class label\n",
        "@symmerty: the symmetry of the predicted class and is an array of 2 elements to represent the symmetry along the y-axis and x-axis\n",
        "\"\"\"\n",
        "class PolylinePrediction:\n",
        "    def __init__(self, original_polylines, polylines, lines_used=1):\n",
        "        self.original_polylines = original_polylines # original polylines\n",
        "        self.polylines = polylines \n",
        "        self.lines_used = lines_used\n",
        "\n",
        "    def add(self, probability, predicted_class, predicted_class_label, lines_used):\n",
        "        self.probability = probability\n",
        "        self.predicted_class = predicted_class\n",
        "        self.predicted_class_label = predicted_class_label\n",
        "        self.lines_used = lines_used\n",
        "    \n",
        "    def add_symmetry(self,symmetry):\n",
        "        self.symmetry = symmetry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rasterizing Pipeline\n",
        "functions to rasterize a polyline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mxIwCaW9NmKY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function to convert a polyline to raster images of dimention = side*side padding and point_diameter are relative to the original 256x256 image.\n",
        "\"\"\"\n",
        "def vector_to_raster(vector_images, side=28, line_diameter=16, padding=16, bg_color=(0,0,0), fg_color=(1,1,1)):\n",
        "    original_side = 256.\n",
        "\n",
        "    surface = cairo.ImageSurface(cairo.FORMAT_ARGB32, side, side)\n",
        "    ctx = cairo.Context(surface)\n",
        "    ctx.set_antialias(cairo.ANTIALIAS_BEST)\n",
        "    ctx.set_line_cap(cairo.LINE_CAP_ROUND)\n",
        "    ctx.set_line_join(cairo.LINE_JOIN_ROUND)\n",
        "    ctx.set_line_width(line_diameter)\n",
        "\n",
        "    # Scale to match the new size\n",
        "    # Add padding at the edges for the point diameter\n",
        "    # and add additional padding to account for antialiasing\n",
        "    total_padding = padding * 2. + line_diameter\n",
        "    new_scale = float(side) / float(original_side + total_padding)\n",
        "    ctx.scale(new_scale, new_scale)\n",
        "    ctx.translate(total_padding / 2., total_padding / 2.)\n",
        "    j = 101\n",
        "    raster_images = []\n",
        "    for vector_image in vector_images:\n",
        "        ctx.set_source_rgb(*bg_color)\n",
        "        ctx.paint()\n",
        "\n",
        "        bbox = np.hstack(vector_image).max(axis=1)\n",
        "        offset = ((original_side, original_side) - bbox) / 2.\n",
        "        offset = offset.reshape(-1,1)\n",
        "        centered = [stroke + offset for stroke in vector_image]\n",
        "\n",
        "        ctx.set_source_rgb(*fg_color)\n",
        "        for xv, yv in centered:\n",
        "            ctx.move_to(xv[0], yv[0])\n",
        "            for x, y in zip(xv, yv):\n",
        "                ctx.line_to(x, y)\n",
        "            ctx.stroke()\n",
        "\n",
        "        data = surface.get_data()\n",
        "        raster_image = np.copy(np.asarray(data)[::4])\n",
        "        raster_image = raster_image.reshape((side, side))\n",
        "        raster_images.append(raster_image)\n",
        "\n",
        "    return raster_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IuoPsgXJgK2O"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function to convert a subset of polylines to raster images of dimention = side*side padding and point_diameter are relative to the original 256x256 image. \n",
        "\"\"\"\n",
        "def vector_to_raster_subset(vector_images, side=28, point_diameter=16, padding=16, bg_color=(0,0,0), fg_color=(1,1,1),line_diameter=0):\n",
        "    original_side = 256.\n",
        "    surface = cairo.ImageSurface(cairo.FORMAT_ARGB32, side, side)\n",
        "    ctx = cairo.Context(surface)\n",
        "    ctx.set_antialias(cairo.ANTIALIAS_BEST)\n",
        "\n",
        "    ctx.set_line_width(point_diameter)\n",
        "\n",
        "    # Scale to match the new size\n",
        "    # Add padding at the edges for the point diameter\n",
        "    # and add additional padding to account for antialiasing\n",
        "    total_padding = padding * 2. + point_diameter\n",
        "    new_scale = float(side) / float(original_side + total_padding)\n",
        "    ctx.scale(new_scale, new_scale)\n",
        "    ctx.translate(total_padding / 2., total_padding / 2.)\n",
        "\n",
        "    raster_images = []\n",
        "    for vector_image in vector_images:\n",
        "        # Clear background\n",
        "        ctx.set_source_rgb(*bg_color)\n",
        "        ctx.paint()\n",
        "\n",
        "        bbox = np.hstack(vector_image).max(axis=1)\n",
        "        offset = ((original_side, original_side) - bbox) / 2.\n",
        "        offset = offset.reshape(-1, 1)\n",
        "        centered = [stroke + offset for stroke in vector_image]\n",
        "\n",
        "        # Draw points\n",
        "        ctx.set_source_rgb(*fg_color)\n",
        "        for xv, yv in centered:\n",
        "            for x, y in zip(xv, yv):\n",
        "                ctx.arc(x, y, point_diameter / 2.0, 0, 2 * np.pi)  # Draw a circle representing the point\n",
        "                ctx.fill()\n",
        "\n",
        "        data = surface.get_data()\n",
        "        raster_image = np.copy(np.asarray(data)[::4])\n",
        "        raster_image = raster_image.reshape((side, side))\n",
        "        raster_images.append(raster_image)\n",
        "\n",
        "    return raster_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Point Simplification and Standardization Pipeline\n",
        "\n",
        "To simplify and standardize the points following steps are done on any polyline:\n",
        "1. Scaling and translating the points to ensure they can fir in a canvas of size 256x256\n",
        "2. Resampling the points to reduce the number of points and ensure that the points are uniformly spaced\n",
        "3. Ramer-Douglas-Peucker algorithm with epsilon=2.0 is used to simplify the points while preserving the original geometry of the polyline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ku-olpItMKai"
      },
      "outputs": [],
      "source": [
        "\"\"\"Funtion to resample the points of the polyline.\"\"\"\n",
        "def resample_points(points, spacing=1.0):\n",
        "    if len(points) < 2:\n",
        "        return points\n",
        "    resampled = [points[0]]\n",
        "    for i in range(1, len(points)):\n",
        "        p1, p2 = resampled[-1], points[i]\n",
        "        dist = np.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)\n",
        "        num_points = int(dist / spacing)\n",
        "        for j in range(1, num_points + 1):\n",
        "            x_new = p1[0] + (p2[0] - p1[0]) * j / (num_points + 1)\n",
        "            y_new = p1[1] + (p2[1] - p1[1]) * j / (num_points + 1)\n",
        "            resampled.append((x_new, y_new))\n",
        "    return resampled\n",
        "\n",
        "\"\"\"Rammer-Douglas-Peucker algorithm to simplify the polyline.\"\"\"\n",
        "def rdp(points, epsilon):\n",
        "    if len(points) < 3:\n",
        "        return points\n",
        "\n",
        "    def perpendicular_distance(point, line_start, line_end):\n",
        "        if line_start == line_end:\n",
        "            return np.linalg.norm(np.array(point) - np.array(line_start))\n",
        "        return np.linalg.norm(np.cross(np.array(line_end) - np.array(line_start), np.array(line_start) - np.array(point))) / np.linalg.norm(np.array(line_end) - np.array(line_start))\n",
        "\n",
        "    def rdp_recursion(points, epsilon):\n",
        "        dmax = 0\n",
        "        index = 0\n",
        "        for i in range(1, len(points) - 1):\n",
        "            d = perpendicular_distance(points[i], points[0], points[-1])\n",
        "            if d > dmax:\n",
        "                index = i\n",
        "                dmax = d\n",
        "        if dmax > epsilon:\n",
        "            results1 = rdp_recursion(points[:index + 1], epsilon)\n",
        "            results2 = rdp_recursion(points[index:], epsilon)\n",
        "            return results1[:-1] + results2\n",
        "        else:\n",
        "            return [points[0], points[-1]]\n",
        "\n",
        "    return rdp_recursion(points, epsilon)\n",
        "\n",
        "\"\"\"Funtion to preprocess and simplify the polyline.\"\"\"\n",
        "def preprocess_points(data):\n",
        "    # Convert to numpy array for easier manipulation\n",
        "    x = np.array(data[0][0])\n",
        "    y = np.array(data[0][1])\n",
        "\n",
        "    # Define SVG canvas size\n",
        "    svg_width = 256\n",
        "    svg_height = 256\n",
        "\n",
        "    # Calculate the bounding box of the points\n",
        "    x_min, x_max = x.min(), x.max()\n",
        "    y_min, y_max = y.min(), y.max()\n",
        "\n",
        "    # Calculate width and height of the bounding box\n",
        "    bbox_width = x_max - x_min\n",
        "    bbox_height = y_max - y_min\n",
        "\n",
        "    # Determine the scaling factor while maintaining aspect ratio\n",
        "    x_scale = svg_width / bbox_width if bbox_width != 0 else 1\n",
        "    y_scale = svg_height / bbox_height if bbox_height != 0 else 1\n",
        "    scale = min(x_scale, y_scale)\n",
        "\n",
        "    # Scale and translate points to fit within the SVG canvas while preserving aspect ratio\n",
        "    scaled_points = [(scale * (xi - x_min) + (svg_width - scale * bbox_width) / 2,\n",
        "                      scale * (yi - y_min) + (svg_height - scale * bbox_height) / 2) for xi, yi in zip(x, y)]\n",
        "\n",
        "    # Resample points to ensure they are evenly spaced\n",
        "    resampled_points = resample_points(scaled_points, spacing=1.0)\n",
        "\n",
        "    # Simplify the polyline using the Ramer-Douglas-Peucker algorithm\n",
        "    epsilon = 2.0\n",
        "    simplified_points = rdp(resampled_points, epsilon)\n",
        "\n",
        "    x_coords, y_coords = zip(*simplified_points)\n",
        "    return [[list(map(int, x_coords)), list(map(int, y_coords))]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing Pipeline\n",
        "Functions to preprocess the data to make it ready for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u1lzK5j2NmKZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"This function resizes the original features X (256*256) into smaller dimensions.\"\"\"\n",
        "def resizing_X(X_orig, small_side):\n",
        "    X = []\n",
        "    for i in range(X_orig.shape[0]):\n",
        "        X.append([])\n",
        "        for j in range(X_orig.shape[1]):\n",
        "            X[-1].append(cv2.resize(X_orig[i, j,], dsize=(small_side, small_side), interpolation=cv2.INTER_CUBIC))\n",
        "    return np.array(X)\n",
        "\n",
        "\"\"\"This function preprocesses the points of the polylines.\"\"\"\n",
        "def preprocessing_pipeline(X,s):\n",
        "\n",
        "    # rasterize the polylines\n",
        "    if s == True:\n",
        "      X = vector_to_raster_subset(X, side=side, padding=16, bg_color=(1,1,1), fg_color=(0,0,0))\n",
        "    else:\n",
        "      # print(X)\n",
        "      X = [preprocess_points(X[0])]\n",
        "      X = vector_to_raster(X, side=side, padding=16, bg_color=(1,1,1), fg_color=(0,0,0))\n",
        "    X = np.array(X)\n",
        "\n",
        "    # If the input is a single sample, add an extra dimension to match batch processing\n",
        "    if len(X.shape) == 3:\n",
        "        X = X.reshape(1, *X.shape)\n",
        "\n",
        "    # dimensionalality reduction\n",
        "    X = resizing_X(X,small_side)\n",
        "\n",
        "    # Flatten the first two dimensions (batch and number of images per batch)\n",
        "    X = X.reshape(X.shape[0] * X.shape[1], small_side, small_side, 1)\n",
        "    X = X.astype('float32') / 255.0\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the trained CNN model\n",
        "To classify a subset of polylines, a Convolutional Neural Network (CNN) model is trained on an augumented version of the QuickDraw dataset. The model is highly efficient, with a compact architecture comprising just 0.6 million parameters, while still achieving a robust accuracy of 93.01% on the test set.\n",
        "\n",
        "The model architecture consists of four Conv2D layers with increasing filter sizes, followed by batch normalization and max-pooling layers to extract and downsample features. The model transitions to fully connected dense layers for classification, using dropout for regularization. The final output layer has 11 units, corresponding to the number of classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEp-JZUcNmKZ",
        "outputId": "bee91eeb-3f98-4e3e-d0f9-2c2dae0e5516"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 32 variables whereas the saved optimizer has 62 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ],
      "source": [
        "# loading the model\n",
        "model = load_model('/content/shape_classifier.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TmD8zx_AC71L"
      },
      "outputs": [],
      "source": [
        "\"\"\"Determines whether a segment is a line or a curve based on the deviation from the actual straight line.\"\"\"\n",
        "def is_line(drawing, threshold=12.0):\n",
        "    # Extract x and y coordinates\n",
        "    x_coords, y_coords = drawing[0]\n",
        "\n",
        "    # Calculate the start and end points\n",
        "    start_point = np.array([x_coords[0], y_coords[0]])\n",
        "    end_point = np.array([x_coords[-1], y_coords[-1]])\n",
        "\n",
        "    # Calculate the line segment vector and its length\n",
        "    line_vector = end_point - start_point\n",
        "    line_length = np.linalg.norm(line_vector)\n",
        "\n",
        "    # Calculate the perpendicular distance from each point to the line segment\n",
        "    distances = []\n",
        "    for (x, y) in zip(x_coords, y_coords):\n",
        "        point = np.array([x, y])\n",
        "        # Vector from start to the point\n",
        "        point_vector = point - start_point\n",
        "        # Projection length of point_vector onto line_vector\n",
        "        proj_length = np.dot(point_vector, line_vector) / line_length\n",
        "        # Closest point on the line segment to the point\n",
        "        closest_point = start_point + proj_length * (line_vector / line_length)\n",
        "        # Calculate the perpendicular distance\n",
        "        distance = np.linalg.norm(point - closest_point)\n",
        "        distances.append(distance)\n",
        "\n",
        "    # Calculate the maximum perpendicular distance\n",
        "    max_distance = max(distances)\n",
        "\n",
        "    # Return whether the segment is a line based on the threshold\n",
        "    return max_distance <= threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction Pipeline\n",
        "\n",
        "The task of predicting the shape was quite complicated as a shape can be represented by one or N polylines. Thus, there is no such way to determine which polyline represents which shape. It is also important consider that one polyline can be a part of multiple shapes. \n",
        "\n",
        "Thus in this ambiguous situation, we experimentally discovered the following approach to predict the shape:\n",
        "1. First, we make classification on every single polyline. Now we consider the following three cases:\n",
        "    1. If the polyline is classified as a shape other than `line` or `curve` with `prediction probability(P) > 0.80`, then we consider it as a shape and add it to final output.\n",
        "    2. If the polyline is classified as a `line` with `prediction probability > 0.70` and the `helper function 'is_line'` which checks if a polyline is line based on the deviation from the actual straight line, returns `True`, then we consider it as a line and add it to a set containing all lines.\n",
        "    3. All other polylines are considered as `curves` and added to a set containing all curves.\n",
        "\n",
        "2. Now, the task can be broken down into finding shapes in the set of `lines` and `curves`. This simplifies the process as a `curve can either be a curve, a circle, or an ellipse`. To deal with this, the only option is to try all subsets of polylines. If we have `N polylines` then we have `2^N subsets`. We can try all subsets and can select the predictions with maximum probability. However, creating all subsets is not feasible for large N and have large memory requirements. \n",
        "\n",
        "3. On further experimentation, we discovered that we can first try the subsets containing more polylines, i.e N, N-1, N-2, ... as they have a higher probability of being a shape. Thus, we first generate `threshold` number of subsets having polylines in decreasing order of N as they have a higher probability of being a shape.\n",
        "\n",
        "4. We then make predictions on these subsets and whenever a shape is predicted with a `probability ϕ > 'tline'` for subset containing only `lines` and `ϕ > 'tcurve'` for subset containing only `curves`, then we add it to the final output. It is important to note that `tline` and `tcurve` are hyperparameters and can be tuned to get better results and the used values are also determined using hit and trial.\n",
        "\n",
        "    1. We now remove the polylines which are part of the predicted shape from the remaining set of polylines and recompute the subsets.\n",
        "\n",
        "    2. We repeat the process until we have no polylines left or no prediction was made in the current iteration. If no prediction was made in the current iteration, we simply add the remaining polylines to the final output as lines or curves.\n",
        "\n",
        "**All the values of thresholds are determined experimetally.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "m42eoP81NmKZ"
      },
      "outputs": [],
      "source": [
        "# thresholds for classifying shapes involving lines and curves\n",
        "tline = 0.958\n",
        "tcurve = 0.968\n",
        "\n",
        "\"\"\"Function to predict the class of a single sample\"\"\"\n",
        "def predict_sample(data, s):\n",
        "    preprocessed_data = preprocessing_pipeline([data],s)\n",
        "    predictions = model.predict(preprocessed_data)\n",
        "    predicted_class = np.argmax(predictions)\n",
        "    probability = np.max(predictions)\n",
        "    return probability, predicted_class\n",
        "\n",
        "\"\"\"Function to create predictions for single samples\"\"\"\n",
        "def create_single_sample_predictions(polylines):\n",
        "    final_output = []\n",
        "    remaining_lines = []\n",
        "    remaining_curves = []\n",
        "\n",
        "    for sample in polylines:\n",
        "        probability, predicted_class = predict_sample(sample.polylines, False)\n",
        "        # print(f'Predicted class: {predicted_class}, Probability: {probability}')\n",
        "        if predicted_class != 0 and probability > 0.80 and ((predicted_class != 7) or (predicted_class == 7 and probability > 0.980)):  # Not a line\n",
        "            sample.add(probability, predicted_class, shape_dict[predicted_class], [1])\n",
        "            final_output.append(sample)\n",
        "        else:  # It’s a line or curve\n",
        "            if predicted_class == 0 and probability > 0.70 and is_line(sample.original_polylines):  # Function to determine if it's a curve\n",
        "                remaining_lines.append(sample)\n",
        "            else:\n",
        "                remaining_curves.append(sample)\n",
        "\n",
        "    return final_output, remaining_lines, remaining_curves\n",
        "\n",
        "\n",
        "\"\"\"Funcrion to generate all possible subsets of polylines to make predictions with maximum number of subsets = threshold\"\"\"\n",
        "# subsets are strategically generated in the above mentioned way\n",
        "def generate_subsequences(data, threshold = 120):\n",
        "    subsequences = []\n",
        "    n = len(data)\n",
        "\n",
        "    # Generate all possible combinations, sorted by number of lines in descending order\n",
        "    combinations = sorted(range(1, 1 << n), key=lambda x: bin(x).count('1'), reverse=True)\n",
        "\n",
        "    for i in combinations:\n",
        "        subset = [[[], []]]\n",
        "        lines = []\n",
        "        for j in range(n):\n",
        "            if i & (1 << j):\n",
        "                subset[0][0].extend(data[j].original_polylines[0][0])\n",
        "                subset[0][1].extend(data[j].original_polylines[0][1])\n",
        "                lines.append(j)  # add j for j'th line\n",
        "        subsequences.append(PolylinePrediction(subset, subset, lines))\n",
        "\n",
        "        # Stop when the threshold number of subsets is reached\n",
        "        if len(subsequences) >= threshold:\n",
        "            break\n",
        "\n",
        "    return subsequences\n",
        "\n",
        "\"\"\"Function to create predictions on subsets\"\"\"\n",
        "def predict_subsets(remaining, final_output, rem_class):\n",
        "\n",
        "    if len(remaining) > 15 :\n",
        "        subsets = []\n",
        "    else :\n",
        "      subsets = generate_subsequences(remaining)\n",
        "    subsets.sort(key=lambda x: len(x.lines_used), reverse=True)  # Sort by number of lines in subset, max first\n",
        "\n",
        "    to_remove = set()  # Set to keep track of all indices to remove\n",
        "\n",
        "    while len(remaining) > 0:\n",
        "        found = 0\n",
        "\n",
        "        for sample in subsets:\n",
        "            probability, predicted_class = predict_sample(sample.polylines, True)\n",
        "            if(predicted_class == 10 and len(sample.lines_used) > 1 and len(sample.lines_used) < 5):\n",
        "                    continue\n",
        "            \n",
        "            # if the samples are curves then they can either be a curve, an ellipse or a circle\n",
        "            # if the samples are lines then they can form the remaining shapes\n",
        "            # They above said logic is implemented below with some experimental thresholds\n",
        "            if (rem_class == 11 and (predicted_class == 1 or predicted_class == 11) and probability > tcurve) or (rem_class == 0 and predicted_class != 0 and probability > tline):\n",
        "                sample.add(probability, predicted_class, shape_dict[predicted_class],sample.lines_used)\n",
        "                final_output.append(sample)\n",
        "\n",
        "                # Mark lines at indices for removal\n",
        "                to_remove.update(sample.lines_used)\n",
        "\n",
        "                # Recalculate the subsets after removal\n",
        "                remaining = [item for i, item in enumerate(remaining) if i not in to_remove]\n",
        "                subsets = generate_subsequences(remaining)\n",
        "                subsets.sort(key=lambda x: len(x.lines_used), reverse=True)\n",
        "\n",
        "                found = 1\n",
        "                break  # Exit the for loop and start over with updated `remaining`\n",
        "        \n",
        "        # No predictions were made and hence the remaining polylines are added as they are\n",
        "        # they can be either lines or curves depending on `rem_class`\n",
        "        if found == 0:\n",
        "            for sample in remaining:\n",
        "                sample.add(1, rem_class, shape_dict[rem_class],sample.lines_used)\n",
        "                final_output.append(sample)\n",
        "\n",
        "            break  # No more subsets to process, exit the loop\n",
        "\n",
        "\"\"\"Function to create predictions on given polylines\"\"\"\n",
        "def create_predictions(polylines):\n",
        "    final_output, remaining_lines, remaining_curves = create_single_sample_predictions(polylines)\n",
        "    predict_subsets(remaining_lines, final_output,0)\n",
        "    predict_subsets(remaining_curves, final_output,11)\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sjHfE9uCNmKa"
      },
      "outputs": [],
      "source": [
        "\"\"\"read data from given csv file\"\"\"\n",
        "def read_csv(csv_path):\n",
        "    np_path_XYs = np.genfromtxt(csv_path, delimiter=',')\n",
        "    path_XYs = []\n",
        "    for i in np.unique(np_path_XYs[:, 0]):\n",
        "        npXYs = np_path_XYs[np_path_XYs[:, 0] == i][:, 1:]\n",
        "        XYs = []\n",
        "        for j in np.unique(npXYs[:, 0]):\n",
        "            XY = npXYs[npXYs[:, 0] == j][:, 1:]\n",
        "            X_cords = XY[:, 0].tolist()\n",
        "            Y_cords = XY[:, 1].tolist()\n",
        "            XYs.append([X_cords, Y_cords])\n",
        "        path_XYs.append(PolylinePrediction(XYs, XYs))\n",
        "    return path_XYs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "duV0IY7wNmKa"
      },
      "outputs": [],
      "source": [
        "\"\"\"Function to create predictions on given csv file\"\"\"\n",
        "def prediction_pipeline(csv_path):\n",
        "    polylines = read_csv(csv_path)\n",
        "    predictions = create_predictions(polylines)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularization pipeline\n",
        "It contains functions to regularize the output of the prediction pipeline. \n",
        "\n",
        "We have created a dedicated funtion to regularize each shape:\n",
        "\n",
        "1. Rectangles: The code calculates the bounding box of the shape and constructs a regularized rectangle based on it, closing the loop to form a complete rectangle.\n",
        "\n",
        "2. Circles: The code calculates a bounding box, finds the center and average diameter, and generates a circular shape using trigonometric functions to create evenly spaced points around the center.\n",
        "\n",
        "3. Stars: The code identifies the bounding box, calculates the center and radius, and alternates between inner and outer radii to generate a regular star shape with evenly spaced points.\n",
        "\n",
        "4. Curves: The code smooths the curve using B-spline interpolation with a higher smoothing factor, producing a smooth curve with a specified number of points.\n",
        "\n",
        "Due to limmitation of time, we cannot implement regularization for all other shapes. However, we smoothen all other shapes using B-spline interpolation with a lower smoothing factor to produce a smoother version of the shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "-IYCYCGtNmKa"
      },
      "outputs": [],
      "source": [
        "\"\"\"Function to regularize rectangles\"\"\"\n",
        "def create_regularized_rectangle(drawing):\n",
        "    x_coords, y_coords = drawing\n",
        "\n",
        "    min_x, max_x = np.min(x_coords), np.max(x_coords)\n",
        "    min_y, max_y = np.min(y_coords), np.max(y_coords)\n",
        "\n",
        "    # Create a rectangle using the bounding box coordinates\n",
        "    rectangle_x = [min_x, max_x, max_x, min_x, min_x]  # Closing the loop\n",
        "    rectangle_y = [min_y, min_y, max_y, max_y, min_y]  # Closing the loop\n",
        "\n",
        "    return [[rectangle_x, rectangle_y]]\n",
        "\n",
        "\"\"\"Function to regularize circles\"\"\"\n",
        "def create_regularized_circle(drawing):\n",
        "    x_coords, y_coords = drawing\n",
        "\n",
        "    min_x, max_x = np.min(x_coords), np.max(x_coords)\n",
        "    min_y, max_y = np.min(y_coords), np.max(y_coords)\n",
        "\n",
        "    # Calculate the length and breadth of the bounding box\n",
        "    length = max_x - min_x\n",
        "    breadth = max_y - min_y\n",
        "\n",
        "    # Calculate the diameter of the circle as the mean of length and breadth\n",
        "    diameter = (length + breadth) / 2\n",
        "\n",
        "    # Calculate the center of the bounding box\n",
        "    center_x = (min_x + max_x) / 2\n",
        "    center_y = (min_y + max_y) / 2\n",
        "\n",
        "    # Number of points to approximate the circle\n",
        "    num_points = 100\n",
        "\n",
        "    # Generate circle coordinates\n",
        "    theta = np.linspace(0, 2 * np.pi, num_points)\n",
        "    circle_x = center_x + (diameter / 2) * np.cos(theta)\n",
        "    circle_y = center_y + (diameter / 2) * np.sin(theta)\n",
        "\n",
        "    return [[circle_x.tolist(), circle_y.tolist()]]\n",
        "\n",
        "\"\"\"Function to regularize star\"\"\"\n",
        "def create_regularized_star(drawing):\n",
        "    x_coords, y_coords = drawing\n",
        "\n",
        "    min_x, max_x = np.min(x_coords), np.max(x_coords)\n",
        "    min_y, max_y = np.min(y_coords), np.max(y_coords)\n",
        "\n",
        "    # Calculate the center and the smaller dimension for the star's radius\n",
        "    center_x = (min_x + max_x) / 2\n",
        "    center_y = (min_y + max_y) / 2\n",
        "    radius = min((max_x - min_x), (max_y - min_y)) / 2\n",
        "\n",
        "    # Calculate star points using polar coordinates\n",
        "    num_points = 5\n",
        "    angle_step = np.pi / num_points\n",
        "    star_x, star_y = [], []\n",
        "\n",
        "    for i in range(2 * num_points):\n",
        "        r = radius if i % 2 == 0 else radius * 0.4  # Alternate between outer and inner radius\n",
        "        angle = i * angle_step\n",
        "        star_x.append(center_x + r * np.cos(angle))\n",
        "        star_y.append(center_y + r * np.sin(angle))\n",
        "\n",
        "    # Close the loop by appending the first point at the end\n",
        "    star_x.append(star_x[0])\n",
        "    star_y.append(star_y[0])\n",
        "\n",
        "    return [[star_x, star_y]]\n",
        "\n",
        "\"\"\"Function to smoothen the curve\"\"\"\n",
        "def regularize_curve(ndjson_object, num_points=10000, s=20.0):\n",
        "    \n",
        "    x, y = ndjson_object[0]\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Calculate the B-spline representation with a higher smoothing factor\n",
        "    tck, u = si.splprep([x, y], s=s, k=3)\n",
        "\n",
        "    # Evaluate the B-spline and generate the smoothed points\n",
        "    u_fine = np.linspace(0, 1, num_points)\n",
        "    x_smooth, y_smooth = si.splev(u_fine, tck)\n",
        "\n",
        "    return [[x_smooth.tolist(), y_smooth.tolist()]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility functions\n",
        "Helper functions for plotting and input-output operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dDx11KSGmYWH"
      },
      "outputs": [],
      "source": [
        "\"\"\"Funtion to plot the regularized shapes\"\"\"\n",
        "def plot_multiple_shapes(shapes):\n",
        "    num_shapes = len(shapes)\n",
        "    cols = 3  # Number of columns in the grid layout\n",
        "    rows = (num_shapes + cols - 1) // cols  # Calculate the number of rows needed\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
        "    axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "    for i, (shape) in enumerate(shapes):\n",
        "        ax = axes[i]\n",
        "        for stroke in shape:\n",
        "            x_coords, y_coords = stroke\n",
        "            ax.plot(x_coords, y_coords, color='black', linewidth=2)\n",
        "        ax.invert_yaxis()  # Invert y-axis to match SVG coordinate system\n",
        "        ax.set_title(f'Shape {i + 1}')\n",
        "        ax.set_xlabel('X-axis')\n",
        "        ax.set_ylabel('Y-axis')\n",
        "        ax.grid(True)\n",
        "\n",
        "    # Turn off axes for any unused subplots\n",
        "    for j in range(num_shapes, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('multiple_shapes.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NweEfcda5xe7"
      },
      "outputs": [],
      "source": [
        "\"\"\"Funtion to create a CSV file from the data as final output\"\"\"\n",
        "def create_csv_from_data(data, csv_filename):\n",
        "    with open(csv_filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Loop through the data and write to CSV\n",
        "        for idx, array in enumerate(data):\n",
        "            x_values, y_values = array[0][0], array[0][1]\n",
        "            for x, y in zip(x_values, y_values):\n",
        "                writer.writerow([idx, 0, x, y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qGuJ_47q_S6a"
      },
      "outputs": [],
      "source": [
        "\"\"\"Function to read csv file and plot the polylines\"\"\"\n",
        "def read_and_plot_csv(csv_path):\n",
        "    np_path_XYs = np . genfromtxt(csv_path , delimiter = ',')\n",
        "    path_XYs = []\n",
        "    for i in np . unique ( np_path_XYs [: , 0]):\n",
        "        npXYs = np_path_XYs [ np_path_XYs [: , 0] == i ][: , 1:]\n",
        "        XYs = []\n",
        "        for j in np . unique ( npXYs [: , 0]):\n",
        "            XY = npXYs [ npXYs [: , 0] == j ][: , 1:]\n",
        "            XYs.append(XY)\n",
        "        path_XYs.append(XYs)\n",
        "\n",
        "    fig , ax = plt . subplots(tight_layout = True , figsize =(8 , 8))\n",
        "    for i , XYs in enumerate (path_XYs):\n",
        "        for XY in XYs :\n",
        "            ax.plot ( XY [: , 0] , XY [: , 1] , linewidth =2)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification Pipeline\n",
        "The flow of main classification pipeline is as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "UsqBYMaFlFBW"
      },
      "outputs": [],
      "source": [
        "\"Main classification pipeline\"\n",
        "def pipeline(csv_path):\n",
        "  \n",
        "    # Plot the original data  \n",
        "    print(\"Original Data\")\n",
        "    read_and_plot_csv(csv_path)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # Predict the classes of the polylines\n",
        "    predictions = prediction_pipeline(csv_path)\n",
        "    for prediction in predictions:\n",
        "        print(f'Predicted class: {prediction.predicted_class_label} with probability {prediction.probability} and polyline {prediction.polylines}')\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # Regularize the shapes on the basis of the predicted classes\n",
        "    regularised_data = []\n",
        "    for prediction in predictions:\n",
        "        if prediction.predicted_class_label == 'square':\n",
        "            prediction.original_polylines = create_regularized_rectangle(prediction.original_polylines[0])\n",
        "        elif prediction.predicted_class_label == 'circle':\n",
        "            prediction.original_polylines = create_regularized_circle(prediction.original_polylines[0])\n",
        "        elif prediction.predicted_class_label == 'star':\n",
        "            prediction.original_polylines = create_regularized_star(prediction.original_polylines[0])\n",
        "        elif prediction.predicted_class_label == 'line':\n",
        "            prediction.original_polylines = regularize_curve(prediction.original_polylines, s=1)\n",
        "        else:\n",
        "            prediction.original_polylines = regularize_curve(prediction.original_polylines, s=1)\n",
        "\n",
        "    for prediction in predictions:\n",
        "        regularised_data.append(prediction.original_polylines)\n",
        "\n",
        "    # Plot the regularized shapes individually\n",
        "    plot_multiple_shapes(regularised_data) \n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # Create a CSV file from the regularized data as final output\n",
        "    create_csv_from_data(regularised_data, 'output.csv')\n",
        "\n",
        "    # Plot the regularized shapes from the CSV file as a final output visualization\n",
        "    read_and_plot_csv('/content/output.csv')   \n",
        "    print()\n",
        "    print() \n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task II: Identification of symmetry in shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Symmetry Detection Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to check the symmetry in the shapes\"\"\"\n",
        "def is_symmetric(data, axis='both', threshold=0.3):\n",
        "    # Extract x and y coordinates\n",
        "    x = np.array(data[0][0])\n",
        "    y = np.array(data[0][1])\n",
        "    result = [0,0]\n",
        "    \n",
        "    # Function to check symmetry\n",
        "    def check_symmetry(coords, mirror_point):\n",
        "        left = coords[:len(coords)//2]\n",
        "        right = coords[len(coords)//2:][::-1]\n",
        "        \n",
        "        if len(left) != len(right):\n",
        "            right = right[1:]\n",
        "        \n",
        "        differences = np.abs(left - (2 * mirror_point - right))\n",
        "        max_deviation = np.max(differences)\n",
        "        \n",
        "        return max_deviation <= threshold * (np.max(coords) - np.min(coords))\n",
        "    \n",
        "    # Check vertical symmetry (around y-axis)\n",
        "    if axis in ['vertical', 'both']:\n",
        "        mirror_x = (np.max(x) + np.min(x)) / 2\n",
        "        if check_symmetry(x, mirror_x):\n",
        "            result[0] = 1\n",
        "    \n",
        "    # Check horizontal symmetry (around x-axis)\n",
        "    if axis in ['horizontal', 'both']:\n",
        "        mirror_y = (np.max(y) + np.min(y)) / 2\n",
        "        if check_symmetry(y, mirror_y):\n",
        "            result[1] = 1\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK1HrR1Coc5s"
      },
      "outputs": [],
      "source": [
        "\"\"\"Function to plot symmetry in the shapes\"\"\"\n",
        "def plot_symmetry(predictions, max_cols=3):\n",
        "    n = len(predictions)\n",
        "    cols = min(n, max_cols)\n",
        "    rows = math.ceil(n / cols)\n",
        "    \n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
        "    fig.tight_layout(pad=3.0)\n",
        "    \n",
        "    if n == 1:\n",
        "        axes = np.array([axes])\n",
        "    \n",
        "    for i, pred in enumerate(predictions):\n",
        "        ax = axes.flat[i] if n > 1 else axes\n",
        "        \n",
        "        # Extract x and y coordinates\n",
        "        x = np.array(pred.original_polylines[0][0])\n",
        "        y = np.array(pred.original_polylines[0][1])\n",
        "        \n",
        "        # Plot the curve\n",
        "        ax.plot(x, y, 'b-', linewidth=2)\n",
        "        ax.scatter(x, y, color='blue', s=20)\n",
        "        \n",
        "        # Plot symmetry lines if applicable\n",
        "        if hasattr(pred, 'symmetry'):\n",
        "            middle_x = (np.max(x) + np.min(x)) / 2\n",
        "            middle_y = (np.max(y) + np.min(y)) / 2\n",
        "            \n",
        "            if pred.symmetry[0] == 1:  # Vertical symmetry\n",
        "                ax.axvline(x=middle_x, color='r', linestyle='--', linewidth=1)\n",
        "                ax.text(middle_x, ax.get_ylim()[1], 'Vertical symmetry', \n",
        "                        rotation=90, va='top', ha='right', fontsize=8, color='r')\n",
        "                \n",
        "            \n",
        "            if pred.symmetry[1] == 1:  # Horizontal symmetry\n",
        "                ax.axhline(y=middle_y, color='g', linestyle='--', linewidth=1)\n",
        "                ax.text(ax.get_xlim()[1], middle_y, 'Horizontal symmetry', \n",
        "                        va='bottom', ha='right', fontsize=8, color='r')\n",
        "        \n",
        "        # Set title and labels\n",
        "        ax.set_xlabel(\"X\", fontsize=8)\n",
        "        ax.set_ylabel(\"Y\", fontsize=8)\n",
        "        \n",
        "        \n",
        "        ax.grid(True, linestyle=':', alpha=0.6)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=8)\n",
        "    \n",
        "    # Remove any unused subplots\n",
        "    for j in range(i+1, rows*cols):\n",
        "        fig.delaxes(axes.flat[j])\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def symmetry_pipeline(regular_data):\n",
        "    for shape in regular_data:\n",
        "        if(shape.predicted_class_label == \"curve\" or shape.predicted_class_label == \"line\"):\n",
        "            sym = is_symmetric(shape.original_polylines)\n",
        "            shape.add_symmetry(sym)\n",
        "        else:\n",
        "            shape.add_symmetry([1,1])\n",
        "\n",
        "    plot_symmetry(regular_data) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main Program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# path to the csv file\n",
        "csv_path = '/frag0.csv'\n",
        "\n",
        "# Run the pipeline\n",
        "regular_data = pipeline(csv_path);\n",
        "\n",
        "#  Symmetry detection\n",
        "symmetry_pipeline(regular_data);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
